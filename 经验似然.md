>[!abstract]+ **Why Empirical Likelihood?**
> 经验似然是一种基于数据驱动的似然比函数的非参数推断方法。与自助法（bootstrap）和删失法（jackknife）类似，经验似然推断不要求我们为数据指定分布族。像参数化似然方法一样，经验似然能够自动确定置信区域的形状；它直接地将通过约束或先验分布表达的附加信息纳入其中；它扩展到偏倚抽样和删失数据，并且具有非常良好的渐近效能特性。可以将经验似然看作是一种不进行重抽样的自助法，也可以看作是一种没有参数假设的似然方法。
# 1. Intro

参数化似然推断的一个主要问题是，我们通常无法确定数据应属于哪个具体的参数化分布族。若选择了错误的分布族，似然估计可能变得低效，甚至导致置信区间和假设检验完全失效。

为避免这种情况，许多统计学家转向非参数推断方法，这些方法无需指定数据的参数化分布。除了经验似然，常见的非参数方法还包括删失法（jackknife）、无穷小删失法（infinitesimal jackknife），以及多种版本的自助法（bootstrap）。这些方法能够提供不依赖于强分布假设的有效置信区间和检验。
## 1.1 偏度与峰度

当一个分布的左右尾部不对称时，我们称其为**偏斜（skewness）**。假设我们观测到随机变量 $X$ 的一组样本值，并计算其均值作为该变量的期望 $\mathbb{E}(X)$，为了量化该随机变量 $X$ 的**偏斜程度**，我们使用下式计算其偏斜系数：
$$\gamma = \frac{\mathbb{E}((X-\mathbb{E}(X))^3)}{\mathbb{E}((X-\mathbb{E}(X))^2)^{3/2}}$$
其中，偏斜系数 $\gamma$ 反映了数据分布的对称性：
- $\gamma < 0$ ：说明变量的分布是左偏的，左尾比右尾更重。
- $\gamma > 0$ ：说明变量的分布是右偏的，右尾比左尾更重。

特别地，对于正态分布等对称分布，偏斜系数 $\gamma=0$ 。

通常，在计算偏斜系数的同时，我们还会计算其**峰度系数（kurtosis）**：
$$\kappa=\frac{\mathbb{E}((X-\mathbb{E}(X))^4)}{\mathbb{E}((X-\mathbb{E}(X))^2)^2} - 3$$
其中，峰度系数 $\kappa$ 反映了数据分布尾部的特性：
- $\kappa < 0$ ：说明数据的尾部比正态分布更“**轻**”，表示数据中存在较少的极端值（离群点）。
- $\kappa > 0$ ：说明数据的尾部比正态分布更“**重**”，表示数据中存在较多的极端值（离群点）。
## 1.2 经验似然、参数化似然、以及自助法（bootstrap）

**参数化似然**方法通过假设数据遵循某种已知分布，并利用最大似然估计（MLE）推断模型参数。通常可以使用**二次逼近**简化计算，将对数似然函数在最大似然估计附近展开，得到易于优化的二次形式。这使得参数化似然在许多标准模型中计算上较为直接，但仍然需要对分布假设有所依赖。

**经验似然**则不依赖于具体的分布假设，它基于样本数据构建似然函数，通过估计方程来定义统计量。在此框架下，经验似然变为一个**凸优化问题**，可以通过**反复最小二乘法**（iterated least squares）轻松找到全局最优解，适用于非参数推断。与参数化似然相比，经验似然计算更灵活，且对分布假设的要求较低，适合于样本量较小或分布未知的情况。

**自助法（bootstrap）** 是一种**重抽样**方法，通过从原始数据中进行多次抽样，生成多个数据集并对每个数据集进行统计推断。自助法的优势在于不依赖分布假设，能够广泛应用于各种统计模型中。虽然它的计算开销较大，但提供了非常灵活和稳健的推断工具，尤其适用于复杂数据结构和小样本情况下。

这些方法可以有效结合使用，增强推断的准确性和计算效率。例如，**经验似然**和**自助法**可以结合，先利用经验似然确定候选置信区间，再通过自助法选择最适合的区间；也可以将**参数化似然**和**经验似然**结合，利用参数化似然估计初始分布，再通过经验似然进一步优化推断结果。这些组合能够在实际问题中提供更强的灵活性和可靠性，特别是在面对复杂数据时。
# 2. 经验似然
## 2.1 无参数最大似然估计（NPMLE）

首先，定义所使用的符号体系：假设随机变量 $X\in\mathbb{R}$ ，其累积分布函数（CDF）表示为 $F(x)=\text{Pr}(X\leq x)$ ，其中 $-\infty < x <\infty$ 。特别地，我们定义 $F(x-)=\text{Pr}(X < x)$ ，因此 $\text{Pr}(X=x)=F(x)-F(x-)$ 。此外，符号 $1_{A(x)}$ 表示在条件 $A(x)$ 为真时取值为 1，否则为 0。
>[!info|wbk wtb tm]+ 
> *定义 2.1*&emsp;设有一组随机变量 $X_1,\ldots,X_n\in\mathbb{R}$ ，其**经验累积分布函数（empirical cumulative distribution function，ECDF）** 定义为
> $$F_n(x)=\frac{1}{n}\sum_{i=1}^n1_{X_i\leq x}$$
> 其中 $-\infty < x < \infty$ 。

>[!info|wbk wtb tm]+ 
> *定义 2.2*&emsp;给定一组随机变量 $X_1,\ldots,X_n\in\mathbb{R}$，假设它们相互独立且具有相同的**真实** CDF $F_0$。对于任意指定的 CDF $F$ ，其对应的非参数似然函数定义为
> $$L(F)=\prod_{i=1}^n\left(F(X_i)-F(X_i-)\right)$$

*定义 2.2* 中给出的似然函数 $L(F)$ 表示在累积分布函数 $F$ 下，观测到样本 $X_1,\ldots,X_n$ 的概率。

通过以下定理，可以证明 *定义 2.1* 中给出的经验累积分布函数（ECDF）即为累积分布函数 $F$ 的无参数最大似然估计（NPMLE）。
>[!info|wbk wtb tm]+ 
> **定理 2.1**&emsp;给定一组随机变量 $X_1,\ldots,X_n\in\mathbb{R}$，假设它们相互独立且具有相同的**真实** CDF $F_0$。令 $F_n$ 为这组随机变量的 ECDF，令 $F$ 为任意指定的 CDF，则有：
> $$L(F)<L(F_n),\quad\mathrm{If}\,F\neq F_n$$
## 2.2 无参似然比

在有参数估计的场景中，我们可以通过**参数似然比**进行假设检验，并确定参数的置信区间。考虑一个参数 $\eta$ 以及其 MLE 估计值 $\hat{\eta}$ ，对于任意值 $\eta_\text{new}$ ，当其对应的参数似然满足 $L(\eta_{\text{new}}) \ll L(\hat{\eta})$ 时，我们有理由拒绝假设 $\eta = \eta_\text{new}$ ，并将 $\eta_\text{new}$ 排除在参数 $\eta$ 的置信区间之外。特别地，通过 Wilk 定理，我们可以确定接受假设的参数似然的下限。如果另有一个参数 $\theta$ ，且 $\theta = \Theta(\eta)$，则可以确定参数 $\theta$ 的置信区间，定义为
$$\{\Theta(\eta)\mid L(\eta)>cL(\hat{\eta})\}$$
其中，系数 $c$ 由 Wilk 定理确定。

类似地，在无参数估计的场景中，我们可以使用**非参数似然比**作为假设检验和置信区间的基础。对于任意指定的分布 $F$ ，定义
$$R(F)=\frac{L(F)}{L(F_n)}$$
其中 $F_n$ 为 NPMLE 估计得到的非参数分布。假设我们关心的是模型中的另一个参数 $\theta$ ，且有 $\theta = T(F)$ ，其中 $T$ 是任意关于分布的**泛函**，$F$ 则属于指定的**分布族** $\mathcal{F}$。于是我们定义参数 $\theta$ 的置信区间为
$$$$